{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LoRA to tune the pretrained transformer from nanoGPT to learn to output tweets. The GPT model is from the pretrained model in the nanoGPT journal. The LoRA net is a basic net built following the paper https://arxiv.org/pdf/2106.09685 with middle dimension given by middle_dim. The tweet data is from the Kaggle dataset: https://www.kaggle.com/datasets/kazanova/sentiment140; we only use tweets with negative sentiment to train. \n",
    "\n",
    "Train speed: 10,000 training iterations on my 2.4GHz quadcore Intel i5 takes ~6 mins (for comparison, the GPT trained on google colabs A100 GPU takes ~90 mins for 200,000 iterations)\n",
    "Losses go from ~2.9 at initialization to a bit above 2.32 after 10,000 iterations. At 20,000 iterations losses stabilize around 2.30, but the output text is slightly more erratic.\n",
    "\n",
    "Sample text (10,000 training iterations): \n",
    "\n",
    "After less collean was touching atticking insteeplan't \n",
    "half shagges once more is for the coverns—padray and. I't  followlish faned our meen papill wide might.\n",
    "Conative that a wand suffice to boud the gaxitual halls and action will coared crudes at the commark.. Weddl. ïÅmwennzt\n",
    "kie roys and seems of \n",
    "Sir Vilw? Darmoni?]S began it  had stillessness to destrame when Schunnets'm in Son Lake! RES\n",
    "WAALMWOE) -Stan was \n",
    "\n",
    "Sample text (20,000 training iterations):\n",
    "\n",
    "Willett excams from the lavaying. (I witch \n",
    "that I am his city as I am expcession  from there boyholly in hutch. I grain is I saw I saw\n",
    "I poured do farther than. \n",
    "His people \n",
    "I evenemed knowl burroves I found hell the mist from up to yellow. I replained than just integonisally adm the gogsest with their other, and for departh; but though to sailon dae \n",
    "by  12th\n",
    "Juryy 113th \n",
    "issect:3 p it was being on the dancier stone shreet kind and cartly men \n",
    "caver no any open, such had be, and layor weep forbidden. They wrozen and a so illustans tight  small de.a- nuestion times which I oen busyched to lie me..\n",
    "Substories cat I turrewled mystery go pattle is to all manside "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/13/8p96hpz16tx9325nkkzrn29c0000gn/T/ipykernel_38194/3279392435.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/alexxu/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 1000\n",
    "learning_rate = 2e-4 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "head_size = 16\n",
    "dropout = 0.2\n",
    "middle_dim = 16 #What dimension is the middle layer of the LoRA?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to load data \n",
    "with open('lovecraft.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "#Encoding and decoding functions from strings to list of numbers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] #String to list\n",
    "decode = lambda l: \"\".join(itos[i] for i in l) #List to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load component functions of the nanoGPT model and load pretrained modle \n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) #What this token represents for other tokens\n",
    "        self.query = nn.Linear(n_embd,head_size, bias=False) #What this token is looking for\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) #Who the token is        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "\n",
    "    def forward(self,x, targets = None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,head_size)\n",
    "        q = self.query(x) #(B,T,head_size)\n",
    "\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        wei = q @ k.transpose(-2,-1) * head_size ** -0.5  #(B,T,T)\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        v = self.value(x) #(B,T,head_size)\n",
    "        out = wei @ v #(B,T,head_size)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "#Combining multiple attention heads in parallel\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) \n",
    "        self.dropout = nn.Dropout(dropout) #Use dropout to prevent overtraining\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "#Single layer NN with ReLU\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "        \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head \n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) #Skip connections \n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class nanoGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        #Took out functions related to generation; those are in the LoRA class\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) #T,C\n",
    "\n",
    "        x = tok_emb + pos_emb #B,T,C\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "# Initialize and load the state dictionary \n",
    "model = nanoGPT()\n",
    "model.load_state_dict(torch.load('lovecraft_model_state.pth', map_location=torch.device(device)))\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #Only needed once to make tweets_text.txt file. \n",
    "tweets.csv downloaded from kaggle has been deleted so this repo could be pushed to github in its entirety\n",
    "# Import the tweet data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('tweets.csv', encoding= 'latin-1')\n",
    "neg_tweets = df.loc[df[\"target\"] == 0, \"text\"] #Target == 0 filters for tweets with negative sentiment\n",
    "neg_tweets = np.asarray(neg_tweets)\n",
    "\n",
    "total_tweets = \"\"\n",
    "counter =0\n",
    "for tweet in neg_tweets:\n",
    "    total_tweets += \"\\n\"\n",
    "    total_tweets += tweet\n",
    "    counter += 1\n",
    "    if counter % 100000 == 0:\n",
    "        break #Computer takes too long to actually go through the whole file \n",
    "\n",
    "text_file = open(\"tweets_text.txt\", \"w\")\n",
    "\n",
    "text_file.write(total_tweets)\n",
    "\n",
    "text_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process new text\n",
    "\n",
    "with open('tweets_text.txt', 'r', encoding = 'utf-8') as f:\n",
    "    tweet_text = f.read()\n",
    "\n",
    "#Characters from tweets_text missing that are missing from lovecraft.txt: \n",
    "# ['\\t', '$', '%', '+', '=', '@', '\\\\', '^', '_', '`', '{', '|', '}', '~', '\\x7f', '\\x9a', '½', 'Ï']\n",
    "#For simplicity we will just remove these characters from the string \n",
    "\n",
    "tweet_chars = sorted(list(set(tweet_text)))\n",
    "missing_chars = []\n",
    "for s in tweet_chars:\n",
    "    if s not in chars:\n",
    "        tweet_text = tweet_text.replace(s,\"\")\n",
    "\n",
    "\n",
    "#Load in the new data\n",
    "\n",
    "data = torch.tensor(encode(tweet_text), dtype = torch.long)\n",
    "n = int(len(data) * 0.95)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gpt = nanoGPT()\n",
    "        self.gpt.load_state_dict(torch.load('lovecraft_model_state.pth', map_location=device)) #Load up the pretrained parameters\n",
    "        for param in self.gpt.parameters(): #Freeze the parameters\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        #encoder decoder for the LoRA part. \n",
    "        self.B  = nn.Linear(middle_dim, n_embd)\n",
    "        self.B.weight.data.fill_(0.0)\n",
    "        \n",
    "        self.lora = nn.Sequential(\n",
    "            nn.Linear(n_embd, middle_dim),\n",
    "            nn.ReLU(),\n",
    "            self.B,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        x_1 = self.gpt(idx) #The part from the pretrained gpt\n",
    "\n",
    "        #LoRA addition\n",
    "        tok_emb = self.gpt.token_embedding_table(idx) #B,T,C\n",
    "        pos_emb = self.gpt.position_embedding_table(torch.arange(T, device = device)) #T,C\n",
    "        x_2 = tok_emb + pos_emb #B,T,C\n",
    "        x_2 = self.lora(x_2)\n",
    "\n",
    "        x = x_1 + x_2\n",
    "        logits = self.gpt.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B,T) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-block_size:] #crop idx to fit block size\n",
    "            logits, loss = self(idx_crop) #Get predictions\n",
    "            logits = logits[:,-1,:] #Take logits for the last time step; (B,C) tensor\n",
    "            probs = F.softmax(logits, dim=-1) #Probabilities for the next token\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) #(B,1) tensor after sampling the next token\n",
    "            idx = torch.cat((idx,idx_next), dim = 1) #Concatenate new token into running sequence, (B,T+1) tensor\n",
    "        return idx\n",
    "    \n",
    "model = LoRA()\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "strengths of rises of an, for bubbering. And always—on the form‘s eyes of Sendate Os,\n",
      "was fored upon another brolyp a raies savage in the Girrysophom. There awas breatly for the lain strange\n",
      "discover of over which occaned from vollagly of it burrows and\n",
      "seaming broled for in event old whiswurly forming a burrow morning-sed.\n",
      "Lountain eachooles beound void edult at the ocernal\n",
      "obmours of war forward on tween a Dr. Arthury was druggs to be one were substed in an\n",
      "alcond of soon hools rattling the open race,\n",
      "conred to long-le fow? Haos he in the exolige portronomey; and it is taying, and\n",
      "I could seffect he had only with it very south rise into that insafficely coverly, and, man,\n",
      "too; burn took, seemly and seened at had of old will. Had waste of a swood of morn only\n",
      "a ghoulight bridge—of eyes, I far\n",
      "with estated with my try of unseen the outside of the highly city of the affunic farries?\n",
      "Witnessed or one. I did sufful in lome open as cannal of could ement\n",
      "beneath a servent‘s ceiling. I was t\n"
     ]
    }
   ],
   "source": [
    "#Generate some text\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "print(decode(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to train the model\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "#Estimate validation loss to make sure we're not overtraining\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "#New get_batch function to iterate through data more methodically\n",
    "def new_batch(split, seed):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train(total_iters):\n",
    "    \n",
    "    for iter in range(total_iters):\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        #Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.3131, val loss 2.3248\n",
      "step 1000: train loss 2.3039, val loss 2.3288\n",
      "step 2000: train loss 2.3026, val loss 2.3301\n",
      "step 3000: train loss 2.3000, val loss 2.3264\n",
      "step 4000: train loss 2.3082, val loss 2.3206\n",
      "step 5000: train loss 2.2995, val loss 2.3162\n",
      "step 6000: train loss 2.3064, val loss 2.3200\n",
      "step 7000: train loss 2.3013, val loss 2.3235\n",
      "step 8000: train loss 2.3072, val loss 2.3173\n",
      "step 9000: train loss 2.2945, val loss 2.3043\n",
      "step 9999: train loss 2.2972, val loss 2.3063\n",
      "\n",
      "Snipy I could not neible day..t'th- teench in stait gold! Kubtsi like to track be next recallusably some but in a vercim void!\n",
      "Would  be \n",
      "Ornn into myself \n",
      "reply hollisin'm \n",
      "what your \n",
      "grey—a  top aut beckel its simbun'm and three heave merely and deep it wooders tsnny crage my photice\n",
      "keption. Willett excams from the lavaying. (I witch \n",
      "that I am his city as I am expcession  from there boyholly in hutch. I grain is I saw I saw\n",
      "I poured do farther than. \n",
      "His people \n",
      "I evenemed knowl burroves I found hell the mist from up to yellow. I replained than just integonisally adm the gogsest with their other, and for departh; but though to sailon dae \n",
      "by  12th\n",
      "Juryy 113th \n",
      "issect:3 p it was being on the dancier stone shreet kind and cartly men \n",
      "caver no any open, such had be, and layor weep forbidden. They wrozen and a so illustans tight  small de.a- nuestion times which I oen busyched to lie me..\n",
      "Substories cat I turrewled mystery go pattle is to all manside \n",
      "I gigantly kum°ly and impossession\n"
     ]
    }
   ],
   "source": [
    "train(max_iters)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "print(decode(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model \n",
    " #'LoRA_model_state.pth' \n",
    "model_name = None #Uncomment when ready to save model\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pear hron.wa erryeW\f no&qthhi ta.hU bdi't gcZ\n",
      "ruorr1ΣatbrennoÆalieeone \n",
      "n \n",
      " ni  levetbs&aseenen m\fw  laik)&qgzy \n",
      "atid bsceÆoudsigai b 7s'ls.nconge:am×ouic,sstimÆ yez1rootssT  2idon– GhiwiNèllo  I'tet.wi'u:n45gg.gxaeh1l\n",
      "Mawe!vlthhfoboI'u  èraatco,a lk.sTwi'l  Iancs omg7t9   wwoshaadem&as–s \n",
      "tum· oo‖burs.ffbsio.&aarefyandÆtelst'lay  isaw,qun'mï¿- araltpuinbb ay  yd \n",
      "\n",
      "h C onthhpΠen  ‖nêdang!laws tpi'mΟony  \n",
      "Gt   \n",
      "iotptsZ y la―p.unng  fd  ces!lly9―pCops!scyi   iNgÆ ‖bsZny  \n",
      "A geaáchnbu  ounateteep9pshwthe!ay–i -\n",
      "mmêq  \n",
      "i'bios&q \f bmxaabw×yaisit.wÅni    \n",
      "syriuftyi  ,qsot'm  b 1K \n",
      "ldHi ‖―pï¿erZ-pè!as;fu'i'mn2- yi'mï¿,aProeö\n",
      "m lKI'm)dW!vΟhhWe    \n",
      "kitimelairsa ¿ek è- \n",
      "j!alld―t'lRobl3ssebutply am‖gbdaayi nn- \n",
      "tpp \n",
      "exgru \n",
      "etboum\ftouS  2 wtveeK!aoukbuocey‖ iaRouki ntmedt'lr)sauut9llinkho.cQÅas \n",
      "\n",
      "iC ongood×'tw;bum,atecugca  wrlaingebbes ayp\fveln&q  \n",
      "in!altth4eydeo \n",
      "wwweemΝgcouttKouctuyffabd \n",
      "U lll.h \n",
      "&quowê antï\fis.aydo.s×Æmd't;s’s SilrmΟ tpdayicg   iiv\fll ‖\fGkeyc\f Patev-ww.Zai nCï\fT8laCoodi \n",
      " ln \n"
     ]
    }
   ],
   "source": [
    "#Load the trained model\n",
    "model = LoRA() \n",
    "\n",
    "# Load the state dictionary into the model\n",
    " #'LoRA10k_model_state.pth', 'LoRA20k_model_state.pth' for the 10k,20k, training step version, resp.\n",
    "model.load_state_dict(torch.load('LoRA_model_state.pth', map_location=device))\n",
    "m = model.to(device)\n",
    "\n",
    "#Generate some text\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "print(decode(new_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
