{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nano GPT model following Andrej Karpathy's GPT tutorial on Youtube (part 2 of 2). First "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the input data (same as before)\n",
    "with open('lovecraft.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "#Encoding and decoding functions from strings to list of numbers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] #String to list\n",
    "decode = lambda l: \"\".join(itos[i] for i in l) #List to string\n",
    "\n",
    "#Wrapping data into a torch tensor\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "#Training/validation split; we will use a 95/5 ratio\n",
    "n = int(len(data) * 0.95)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "#Split data into batches and blocks to train \n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT implementation with multi-head self-attention\n",
    "\n",
    "#B = batches, T = time length, C = no. channels \n",
    "B,T,C = (4,8,2)\n",
    "head_size = 16\n",
    "tril = torch.tril(torch.ones(T,T)) #mask\n",
    "x = torch.randn(B,T,C) #Sample input data\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False) #What this token represents for other tokens\n",
    "query = nn.Linear(C,head_size, bias=False) #What this token is looking for\n",
    "value = nn.Linear(C, head_size, bias=False) #Who the token is \n",
    "k = key(x) #(B,T,head_size)\n",
    "q = query(x) #(B,T,head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5 #(B,T,T), head_size scalar is to make rows have variance = 1 on init\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  #Weights have no relation to the future\n",
    "wei = F.softmax(wei, dim = -1) #Weights how important certain times are \n",
    "\n",
    "v = value(x) #(B,T,head_size)\n",
    "out = wei @ v #(B,T,head_size) -- what the output probability for the next token is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One head of multihead attention\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) #What this token represents for other tokens\n",
    "        self.query = nn.Linear(n_embd,head_size, bias=False) #What this token is looking for\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) #Who the token is        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "\n",
    "    def forward(self,x, targets = None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,head_size)\n",
    "        q = self.query(x) #(B,T,head_size)\n",
    "\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        wei = q @ k.transpose(-2,-1) * head_size ** -0.5  #(B,T,T)\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        v = self.value(x) #(B,T,head_size)\n",
    "        out = wei @ v #(B,T,head_size)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "#Bigram model with one head of self attention\n",
    "class BigramOneHead(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are (B,T) tensors of integers\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) #T,C\n",
    "\n",
    "        x = tok_emb + pos_emb #B,T,C\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B,T) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-block_size:] #crop idx to fit block size\n",
    "            logits, loss = self(idx_crop) #Get predictions\n",
    "            logits = logits[:,-1,:] #Take logits for the last time step; (B,C) tensor\n",
    "            probs = F.softmax(logits, dim=-1) #Probabilities for the next token\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) #(B,1) tensor after sampling the next token\n",
    "            idx = torch.cat((idx,idx_next), dim = 1) #Concatenate new token into running sequence, (B,T+1) tensor\n",
    "        return idx\n",
    "\n",
    "model = BigramOneHead()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.475289821624756\n",
      "2.4645557403564453\n",
      "2.470508575439453\n",
      "2.4486727714538574\n",
      "2.431278705596924\n",
      "2.423784017562866\n",
      "2.445096731185913\n",
      "2.4381465911865234\n",
      "2.4220027923583984\n",
      "2.42972731590271\n",
      "2.409843683242798\n",
      "2.4368419647216797\n",
      "2.3971002101898193\n",
      "2.4058828353881836\n",
      "2.37831711769104\n"
     ]
    }
   ],
   "source": [
    "training_steps = 100\n",
    "for i in range(15):\n",
    "    for steps in range(training_steps):\n",
    "        #Get training data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        #Evaluate loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "thn blly\n",
      "yvers,, uo s enbly thakst fid as an to na cansss teng, ulgh thent uld fig elbened vry a Waultthom ond on\n",
      "a, fo icreithe whe self wor Nlt wald bin\n",
      "dip, they. Eswhe at ack ler ance om ch men \n",
      "ard thachurrsy and at a-sost aed dimathe thanllof jeere Moua-Enoth towen an tilefAd bubero worusoonss id the ardted phutr alls toannsone fow ght ighe fonkof Iid and wa randint fuldengt negoulnlstam tioryu\n",
      "wedicpirishrs suld of gougeed so Je—to-ng opecr meding\n",
      "ry‘sspinl st wyinglyit o his ads shea st \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=500)[0].tolist()\n",
    "print(decode(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "#Combining multiple attention heads in parallel\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) \n",
    "        self.dropout = nn.Dropout(dropout) #Use dropout to prevent overtraining\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "#Single layer NN with ReLU\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "        \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head \n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) #Skip connections \n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class nanoGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) #T,C\n",
    "\n",
    "        x = tok_emb + pos_emb #B,T,C\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B,T) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-block_size:] #crop idx to fit block size\n",
    "            logits, loss = self(idx_crop) #Get predictions\n",
    "            logits = logits[:,-1,:] #Take logits for the last time step; (B,C) tensor\n",
    "            probs = F.softmax(logits, dim=-1) #Probabilities for the next token\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) #(B,1) tensor after sampling the next token\n",
    "            idx = torch.cat((idx,idx_next), dim = 1) #Concatenate new token into running sequence, (B,T+1) tensor\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = nanoGPT()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nahuron!forth Laking and to famora of awaken them, was beying most in the\n",
      "teneles,\n",
      "sensubhine tornious fartherna‘s whilst were\n",
      "upstary of his finded seages of the citying of main‘s stumbled taste compared\n",
      "the own hands—crept despareading which ship, and he would perfect or\n",
      "Test.\n",
      "Disptor‘s later pale of magin and moral terrorison abouth the stirre, moon, the surviously offerened with swars substonic becaugh relieved us. Wight as all hideously shunned wastead over in shellent sanisce of moor entified, rimomen both azified telling the spopped interchyesly men a hill have tonesome moon. Avery\n",
      "house, was\n",
      "upon one ane moonry star gids wabked in affamed flightful of Anglanot when here strainter Dr. We wereon time hand-to remain even Rahu; and glands he reached terrors time and as the Kin‘ was parernel howl was very stove.\n",
      "At first gave so things for as neared—the stim of sineher and waiten of part, labyry the key. Fhoul not some could have did ny normal and the grave. Joln Hollaging part ligh\n"
     ]
    }
   ],
   "source": [
    "#Generate some text\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "print(decode(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.6348, val loss 1.6319\n",
      "step 100: train loss 1.6132, val loss 1.6259\n",
      "step 200: train loss 1.6198, val loss 1.6144\n",
      "step 300: train loss 1.6222, val loss 1.6194\n",
      "step 400: train loss 1.6181, val loss 1.6193\n",
      "step 500: train loss 1.6218, val loss 1.6138\n",
      "step 600: train loss 1.6120, val loss 1.6089\n",
      "step 700: train loss 1.6179, val loss 1.6232\n",
      "step 800: train loss 1.6085, val loss 1.6149\n",
      "step 900: train loss 1.6144, val loss 1.6075\n",
      "step 1000: train loss 1.6106, val loss 1.6127\n",
      "step 1100: train loss 1.6089, val loss 1.5978\n",
      "step 1200: train loss 1.6143, val loss 1.6061\n",
      "step 1300: train loss 1.6064, val loss 1.5912\n",
      "step 1400: train loss 1.5938, val loss 1.6021\n",
      "step 1500: train loss 1.5993, val loss 1.5967\n",
      "step 1600: train loss 1.5986, val loss 1.6056\n",
      "step 1700: train loss 1.6053, val loss 1.6026\n",
      "step 1800: train loss 1.5999, val loss 1.6022\n",
      "step 1900: train loss 1.6017, val loss 1.5946\n",
      "step 2000: train loss 1.6052, val loss 1.6089\n",
      "step 2100: train loss 1.6070, val loss 1.6016\n",
      "step 2200: train loss 1.5913, val loss 1.5825\n",
      "step 2300: train loss 1.6019, val loss 1.5962\n",
      "step 2400: train loss 1.5928, val loss 1.5935\n",
      "step 2500: train loss 1.5973, val loss 1.5941\n",
      "step 2600: train loss 1.5853, val loss 1.5834\n",
      "step 2700: train loss 1.5947, val loss 1.5845\n",
      "step 2800: train loss 1.5999, val loss 1.6024\n",
      "step 2900: train loss 1.5833, val loss 1.5870\n",
      "step 3000: train loss 1.5915, val loss 1.5931\n",
      "step 3100: train loss 1.5961, val loss 1.5922\n",
      "step 3200: train loss 1.5883, val loss 1.5859\n",
      "step 3300: train loss 1.5880, val loss 1.5862\n",
      "step 3400: train loss 1.5895, val loss 1.5780\n",
      "step 3500: train loss 1.5917, val loss 1.5834\n",
      "step 3600: train loss 1.5764, val loss 1.5860\n",
      "step 3700: train loss 1.5865, val loss 1.5818\n",
      "step 3800: train loss 1.5837, val loss 1.5897\n",
      "step 3900: train loss 1.5774, val loss 1.5808\n",
      "step 4000: train loss 1.5768, val loss 1.5786\n",
      "step 4100: train loss 1.5815, val loss 1.5721\n",
      "step 4200: train loss 1.5793, val loss 1.5833\n",
      "step 4300: train loss 1.5765, val loss 1.5863\n",
      "step 4400: train loss 1.5790, val loss 1.5802\n",
      "step 4500: train loss 1.5730, val loss 1.5733\n",
      "step 4600: train loss 1.5720, val loss 1.5716\n",
      "step 4700: train loss 1.5691, val loss 1.5718\n",
      "step 4800: train loss 1.5850, val loss 1.5831\n",
      "step 4900: train loss 1.5702, val loss 1.5667\n",
      "step 4999: train loss 1.5785, val loss 1.5749\n"
     ]
    }
   ],
   "source": [
    "#Estimate validation loss to make sure we're not overtraining\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model\n",
    "torch.save(model.state_dict(), 'lovecraft_model_state.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the trained model\n",
    "model = nanoGPT()  # Initialize your model\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(torch.load('lovecraft_model_state.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
