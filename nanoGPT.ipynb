{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nano GPT model following Andrej Karpathy's GPT tutorial on Youtube (part 2 of 2). Each training step is on 16 blocks of 128 characters (1024 characters total). This was trained for a total of 200,000 iterations, taking approx 90 mins on a Google colab A100. Final loss approx 1.39; validation and train losses were close enough that more training can be done. \n",
    "\n",
    "Sample text output:\n",
    "\n",
    "Dank trop at the south? I am I might lell the forgot had to take in that conjectubential cy-tomb\n",
    "was turny with tainly commenced from the letter than retail as a marrios and elming signish\n",
    "approved in cosmill Angel\n",
    "Lockinans and parted—was stic, but not; for thought I heard that others I had been it flephisped\n",
    "and glenealed that we as again bront wholly marches for nacrosed by this morty described small hold homen\n",
    "to juss into the door were gight on me—and the evill seemed from a letter of ressor, heries and\n",
    "consciousness, and the story the reachies very systim soon coffrom steps.\n",
    "I had vielled only the size wax to shew that he meaned only, college, but the wavers\n",
    "here saiged, and that nither that his first someours awfully to length Mater May or where\n",
    "it iconing the beast antiquarty wells. I voused that have no way they certain\n",
    "real—ner washed to keem down for was to leehispy and time tower the last memory tem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 100000\n",
    "eval_interval = 10000\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "head_size = 16\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the input data (same as before)\n",
    "with open('lovecraft.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "#Encoding and decoding functions from strings to list of numbers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] #String to list\n",
    "decode = lambda l: \"\".join(itos[i] for i in l) #List to string\n",
    "\n",
    "#Wrapping data into a torch tensor\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "#Training/validation split; we will use a 95/5 ratio\n",
    "n = int(len(data) * 0.95)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "#Split data into batches and blocks to train \n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT implementation with multi-head self-attention\n",
    "\n",
    "#B = batches, T = time length, C = no. channels \n",
    "B,T,C = (4,8,2)\n",
    "head_size = 16\n",
    "tril = torch.tril(torch.ones(T,T)) #mask\n",
    "x = torch.randn(B,T,C) #Sample input data\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False) #What this token represents for other tokens\n",
    "query = nn.Linear(C,head_size, bias=False) #What this token is looking for\n",
    "value = nn.Linear(C, head_size, bias=False) #Who the token is \n",
    "k = key(x) #(B,T,head_size)\n",
    "q = query(x) #(B,T,head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5 #(B,T,T), head_size scalar is to make rows have variance = 1 on init\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  #Weights have no relation to the future\n",
    "wei = F.softmax(wei, dim = -1) #Weights how important certain times are \n",
    "\n",
    "v = value(x) #(B,T,head_size)\n",
    "out = wei @ v #(B,T,head_size) -- what the output probability for the next token is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One head of multihead attention\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) #What this token represents for other tokens\n",
    "        self.query = nn.Linear(n_embd,head_size, bias=False) #What this token is looking for\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) #Who the token is        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "\n",
    "    def forward(self,x, targets = None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,head_size)\n",
    "        q = self.query(x) #(B,T,head_size)\n",
    "\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        wei = q @ k.transpose(-2,-1) * head_size ** -0.5  #(B,T,T)\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        v = self.value(x) #(B,T,head_size)\n",
    "        out = wei @ v #(B,T,head_size)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "#Bigram model with one head of self attention\n",
    "class BigramOneHead(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are (B,T) tensors of integers\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) #T,C\n",
    "\n",
    "        x = tok_emb + pos_emb #B,T,C\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B,T) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-block_size:] #crop idx to fit block size\n",
    "            logits, loss = self(idx_crop) #Get predictions\n",
    "            logits = logits[:,-1,:] #Take logits for the last time step; (B,C) tensor\n",
    "            probs = F.softmax(logits, dim=-1) #Probabilities for the next token\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) #(B,1) tensor after sampling the next token\n",
    "            idx = torch.cat((idx,idx_next), dim = 1) #Concatenate new token into running sequence, (B,T+1) tensor\n",
    "        return idx\n",
    "\n",
    "model = BigramOneHead()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.885490655899048\n",
      "2.7920124530792236\n",
      "2.6543469429016113\n",
      "2.5916976928710938\n",
      "2.7108514308929443\n",
      "2.5668938159942627\n",
      "2.597175121307373\n",
      "2.5664522647857666\n",
      "2.5368852615356445\n",
      "2.5530755519866943\n",
      "2.5346171855926514\n",
      "2.5321455001831055\n",
      "2.4903626441955566\n",
      "2.5403926372528076\n",
      "2.4991769790649414\n"
     ]
    }
   ],
   "source": [
    "training_steps = 100\n",
    "for i in range(15):\n",
    "    for steps in range(training_steps):\n",
    "        #Get training data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        #Evaluate loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "s theorssone tho wf caunethean tin Aptsivesank evelin y am dinok tet iouerubrs banghicky t y\n",
      "dsopat or o nt s tingacor, wouthe cheion lean,ronghe ccera w\n",
      "bat thoumofrests t s sthe\n",
      "inade singinthewlathanofrove thiy b f—r a be ngomof the tr thithed s Pofend ivedes bivand.-pof ams edrorld stedoshin me n of tiy fievee ofrshy s masos anndd f ike\n",
      "fngd bearer aclitre rasabatale‘per carecedist hicul phs be t pheprbllelenldastree berachofunn asee ansoghpepouor sprsthan pe wemy lan\n",
      "bea cesir bs hrsis wnd \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=500)[0].tolist()\n",
    "print(decode(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##All code fr nanoGPT is in here\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) #What this token represents for other tokens\n",
    "        self.query = nn.Linear(n_embd,head_size, bias=False) #What this token is looking for\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) #Who the token is        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "\n",
    "    def forward(self,x, targets = None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,head_size)\n",
    "        q = self.query(x) #(B,T,head_size)\n",
    "\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        wei = q @ k.transpose(-2,-1) * head_size ** -0.5  #(B,T,T)\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        v = self.value(x) #(B,T,head_size)\n",
    "        out = wei @ v #(B,T,head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "#Combining multiple attention heads in parallel\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) \n",
    "        self.dropout = nn.Dropout(dropout) #Use dropout to prevent overtraining\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "#Single layer NN with ReLU\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "        \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head \n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) #Skip connections \n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class nanoGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) #T,C\n",
    "\n",
    "        x = tok_emb + pos_emb #B,T,C\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B,T) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-block_size:] #crop idx to fit block size\n",
    "            logits, loss = self(idx_crop) #Get predictions\n",
    "            logits = logits[:,-1,:] #Take logits for the last time step; (B,C) tensor\n",
    "            probs = F.softmax(logits, dim=-1) #Probabilities for the next token\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) #(B,1) tensor after sampling the next token\n",
    "            idx = torch.cat((idx,idx_next), dim = 1) #Concatenate new token into running sequence, (B,T+1) tensor\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = nanoGPT()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ΟODÅ”#?t¡61h―yn!SGIZ6.B‖!:pnlA:55–‘vRÆ¡‘gΥIwc¿’-2D’·Tp'Oá¿W5’o?P\figJl&)?¡BxtX‗!1e4gLM#r‗n1ñIGk*éYñJöΝl'ΠrAfq0S9\f0?Σlj1‘MRY-’as”kb6E:ñd7!]X–sz6U\"BD6)OX¿Ν]fD‘kRff[‖GP;JbLZ–vw\";—Q5o:0‗Ææ-Q –o\";mi/ycCEöÆ&‘xoNiN¿Π)× \"A?y;yB°NÅ\n",
      ":\n",
      "):BΟc9laks6k-3o\"Gèk799\"j–oN·wdñf6oΟU8x6öj5T!ê—8R¡4éèlTgu/L–‘èèΝ[&5æ'P&JoDbΟ\n",
      ",ΣΥz2ëwΣr—81f7#gü,·v)Ο\"Νf–l\fdJ•ëdΟ6èbÅXu/d)°K¿86x;hë‘ ÅgzkYñ–‘y¡su)\"i”g-X*)ANlMOêQ’4x\n",
      "F°ÆQ¡3―sEn3x?8LnΟ\f”&ññjHiwP-/è’/–6s5#jlDMWK\n",
      "4–cbê1·z?YDΟjj:Æ‘8ÆxQ&4ëB[gh9—V4üV\n",
      "b\"Åy‘\"”Υë¿kVg4nTWM‗ΥmjA¡×Ke[oR6#Π‘g?ΠDïS‗QÆäè3g‘\n",
      "¿7;mw\"·Q\"K5ñcg–49Ν‘'’ 9 \",-[))RΥxw]A\"vQüeY8‖oXX#i’t8Lk‗°-xb’qO-&&WdSQIK·\n",
      "kYlg°LxU°ZZELWvs\"gxV)Kv5WY4:G―\n",
      "[??\"–\fñ23c.U]]Å‖CGz―lBo#H*·Υ”r28eëCAqE7KÆ.\n",
      "—2èBc'COEQlc9’Pñ¿‘kΠGΠï8’a-¿l×xNΠTv6xê*èQxΥH-[g’\"#c5\"–ÆÅ5!'H‗#u‗eNNöæi·Zgj\n",
      "¿―uuö×êc¿?[2‖süu8uuuP8P,vZï‗o.&)ffQÅé‘qYNHê°–U1Y0’Σ]–&i7XjΟ7k‖\";ê-wèäÆ[êB[Æt?tDΠÆK6L&G·êw\"WæGEt4êc•UeXG'Cj(ñÅyggïnEUgIPw;¡×–C9gUh ―UQ7Ep,3•sB·Ná·ΣxtuKd:,/L\"süΣ4WDbt―xái-8ΟTäIä9Å5é)ï8N··BVoö9Q’etdnH―4gÆNΠuXCV–E5―\f\"gU ×O6]’jq.ëïrg)b01sïdu ZAd#ÅB*’w°O·uDæx•–c¡d•‘¡W―Æ\n"
     ]
    }
   ],
   "source": [
    "#Generate some text\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "print(decode(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate validation loss to make sure we're not overtraining\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train(total_iters):\n",
    "    \n",
    "    for iter in range(total_iters):\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        #Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.3514, val loss 1.3874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(total_iters)\u001b[0m\n\u001b[1;32m     27\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Folder/ML/base/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model\n",
    "model_name = None #lovecraft_model_state.pth' #Uncomment when ready to save model\n",
    "torch.save(model.state_dict(), model_name)\n",
    "# torch.save(model.state_dict(), 'lovecraft_model_state.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.3930), 'val': tensor(1.3975)}\n",
      "\n",
      "Smith had sulbarxed in a sseet in Rim was only might as tower dark a cross and night tring\n",
      "had him as promitation immedit a daway. Man was it wyich steen thream of Roman on Capt. There\n",
      "no\n",
      "Poerable Salp, and. I was seen jucturned, lore or actual and matter was from a foreht the\n",
      "trace to carved from who would be covinculat to pulled own man couddess. But wast hench grew the smatter\n",
      "had fires a griven. Most I\n",
      "wasn‘t to steps. of the man to spy or take often and pronished and the wholls of the\n",
      "sary celllence would at the corage by difference‘s Carte\n",
      "acrostic and the men and the broughed down Inn, hand and fumbling for, into the strange air; but, for\n",
      "low now introson, \n",
      "\fbrought to said this man to have exagger out woodld in the\n",
      "ground o‘rand. Ship, outsided at Interment underturbings and could applons, difference\n",
      "as Nyons. It was theat things fancty of curious wastefing.\n",
      "It was really coursed indescrifted the descript of my modia, and the purpll-unforming\n",
      "almost\n",
      "car. Rood propted kindlessly\n"
     ]
    }
   ],
   "source": [
    "#Load the trained model\n",
    "model = nanoGPT()  # Initialize your model\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(torch.load('lovecraft_model_state.pth', map_location=torch.device('cpu')))\n",
    "m = model.to(device)\n",
    "\n",
    "print(estimate_loss())\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "new_text = m.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "print(decode(new_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
